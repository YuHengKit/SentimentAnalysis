{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis with CLS Token from BERT with SST Dataset\n",
    "\n",
    "We will train with SST dataset and test with 2000+2000 SST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "#import necessary library & settings\n",
    "import torch\n",
    "import transformers\n",
    "from transformers import BertModel, BertTokenizer, AdamW, get_linear_schedule_with_warmup\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from pylab import rcParams\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rc\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from collections import defaultdict\n",
    "from textwrap import wrap\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import warnings\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format='retina'\n",
    "sns.set(style='whitegrid', palette='muted', font_scale=1.2)\n",
    "#HAPPY_COLORS_PALETTE = [\"#01BEFE\", \"#FFDD00\", \"#FF7D00 \", \"#FF006D\", \"#ADFF02\", \"#8F00FF\"]\n",
    "#sns.set_palette(sns.color_palette(HAPPY_COLORS_PALETTE))\n",
    "rcParams['figure.figsize'] = 12, 8\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's understand the classes distribution for SST dataset\n",
    "\n",
    "### We select the first 8000 English and Malay dataset for experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# Read train data\n",
    "column_names = [\"sentence\",\"label\"]\n",
    "df = pd.DataFrame(columns = column_names)\n",
    "df_en = pd.read_csv('C:/Users/Yuheng/sst_train.txt', sep='\\t', header=None, names=['truth', 'text'],encoding='latin-1')\n",
    "df_en = df_en[:8000]\n",
    "df_en['truth'] = df_en['truth'].str.replace('__label__', '')\n",
    "df_en['truth'] = df_en['truth'].astype(int).astype('category')\n",
    "df_en['sentence']=df_en['text']\n",
    "df_en['label']=df_en['truth']\n",
    "\n",
    "df_malay = pd.read_csv('Desktop/sst-5_malay.csv', header=0, names=['sentence', 'label'],encoding='latin-1')\n",
    "df_malay = df_malay[:8000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We want only 3 classes, so we will combine related classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_sentiment(rating):\n",
    "  rating = int(rating)\n",
    "  if rating == 1:\n",
    "    return 0\n",
    "  elif rating == 2:\n",
    "    return 0\n",
    "  elif rating == 3:\n",
    "    return 1\n",
    "  elif rating == 4:\n",
    "    return 2\n",
    "  else:\n",
    "    return 2\n",
    "df_en['label'] = df_en.label.apply(to_sentiment)\n",
    "class_names = ['Negative','Neutral','Positive']\n",
    "df_malay['label']=df_malay.label.apply(to_sentiment)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now, we load mBERT and put it into GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "PRE_TRAINED_MODEL_NAME = 'bert-base-multilingual-cased'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(PRE_TRAINED_MODEL_NAME)\n",
    "bert_model = BertModel.from_pretrained(PRE_TRAINED_MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = ['Negative','Neutral', 'Positive']\n",
    "#model = SentimentClassifier(len(class_names))\n",
    "model = bert_model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First round of dataset splitting to get testing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features_en, test_features_en, train_labels_en, test_labels_en = train_test_split(df_en['sentence'], df_en['label'], random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features_malay, test_features_malay, train_labels_malay, test_labels_malay = train_test_split(df_malay['sentence'], df_malay['label'], random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We obtain the CLS token from mBERT for every sentence in the dataset as our embeddings and saved into \"encoded\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract CLS token for every sentence\n",
    "encoded_en=[]\n",
    "for i in range (0,len(df_en['sentence'])):\n",
    "    encoded_review = tokenizer.encode_plus(\n",
    "      df_en['sentence'][i],\n",
    "      max_length=512,\n",
    "      truncation=True,\n",
    "      add_special_tokens=True,\n",
    "      return_token_type_ids=False,\n",
    "      pad_to_max_length=True,\n",
    "      return_attention_mask=True,\n",
    "      return_tensors='pt',\n",
    "    )\n",
    "    input_ids = encoded_review['input_ids'].to(device)\n",
    "    attention_mask = encoded_review['attention_mask'].to(device)\n",
    "    with torch.no_grad():\n",
    "        last_hidden_states = model(input_ids, attention_mask=attention_mask)\n",
    "    feature = last_hidden_states\n",
    "    features = (feature[0][:,0,:].cpu()).numpy().flatten() #extract the last CLS token from BERT-layer and flatten into 1d array\n",
    "    encoded_en.append(features)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract CLS token for every sentence\n",
    "encoded_malay=[]\n",
    "for i in range (0,len(df_malay['sentence'])):\n",
    "    encoded_review = tokenizer.encode_plus(\n",
    "      df_malay['sentence'][i],\n",
    "      max_length=512,\n",
    "      truncation=True,\n",
    "      add_special_tokens=True,\n",
    "      return_token_type_ids=False,\n",
    "      pad_to_max_length=True,\n",
    "      return_attention_mask=True,\n",
    "      return_tensors='pt',\n",
    "    )\n",
    "    input_ids = encoded_review['input_ids'].to(device)\n",
    "    attention_mask = encoded_review['attention_mask'].to(device)\n",
    "    with torch.no_grad():\n",
    "        last_hidden_states = model(input_ids, attention_mask=attention_mask)\n",
    "    feature = last_hidden_states\n",
    "    features = (feature[0][:,0,:].cpu()).numpy().flatten() #extract the last CLS token from BERT-layer and flatten into 1d array\n",
    "    encoded_malay.append(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Don't forget to pass the sentiment to labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_en=df_en['label']\n",
    "labels_malay=df_malay['label']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifier Training\n",
    "#### Second round splitting for classifier training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features_new_en, test_features_new_en, train_labels_new_en, test_labels_new_en = train_test_split(encoded_en, labels_en, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features_new_malay, test_features_new_malay, train_labels_new_malay, test_labels_new_malay = train_test_split(encoded_malay, labels_malay, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combine testing data (2000+2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = [\"column\",\"label\"]\n",
    "df_test_new = pd.DataFrame(test_labels_en,columns = column_names)\n",
    "df_test_new['sentence']=test_features_en\n",
    "#df_test_new['label']=test_labels_en\n",
    "df_test_new=df_test_new.drop(columns=[\"column\"])\n",
    "df_test_new['lan']=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = [\"column\",\"label\"]\n",
    "df_test_new_malay = pd.DataFrame(test_labels_malay,columns = column_names)\n",
    "df_test_new_malay['sentence']=test_features_malay\n",
    "#df_test_new['label']=test_labels_en\n",
    "df_test_new_malay=df_test_new_malay.drop(columns=[\"column\"])\n",
    "df_test_new_malay['lan']=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4000 entries, 0 to 3999\n",
      "Data columns (total 3 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   label     4000 non-null   int64 \n",
      " 1   sentence  4000 non-null   object\n",
      " 2   lan       4000 non-null   int64 \n",
      "dtypes: int64(2), object(1)\n",
      "memory usage: 93.9+ KB\n"
     ]
    }
   ],
   "source": [
    "df_test_new=df_test_new.append(df_test_new_malay,ignore_index = True)\n",
    "df_test_new.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>sentence</th>\n",
       "      <th>lan</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3995</th>\n",
       "      <td>0</td>\n",
       "      <td>Blue Crush begitu berpanjangan dan membosankan...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3996</th>\n",
       "      <td>2</td>\n",
       "      <td>Kelegaan dari filem besbol yang terlalu keras ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3997</th>\n",
       "      <td>1</td>\n",
       "      <td>Orang miskin Stuart memerlukan sejumlah sinis ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3998</th>\n",
       "      <td>0</td>\n",
       "      <td>Tidak ada jumlah pembakaran, peledakan, tusuka...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3999</th>\n",
       "      <td>2</td>\n",
       "      <td>Berjaya mencapai apa yang dapat dilakukan oleh...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      label                                           sentence  lan\n",
       "3995      0  Blue Crush begitu berpanjangan dan membosankan...    1\n",
       "3996      2  Kelegaan dari filem besbol yang terlalu keras ...    1\n",
       "3997      1  Orang miskin Stuart memerlukan sejumlah sinis ...    1\n",
       "3998      0  Tidak ada jumlah pembakaran, peledakan, tusuka...    1\n",
       "3999      2  Berjaya mencapai apa yang dapat dilakukan oleh...    1"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test_new.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's train our English classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6075\n",
      "--- 19.230565786361694 seconds ---\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "import time\n",
    "start_time = time.time()\n",
    "svc_clf = make_pipeline(StandardScaler(), SVC(gamma='auto'))\n",
    "svc_clf.fit(train_features_new_en,train_labels_new_en)\n",
    "svc_score=svc_clf.score(test_features_new_en, test_labels_new_en)\n",
    "print(svc_score)\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.594\n",
      "--- 4.811445236206055 seconds ---\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.svm import SVC\n",
    "import time\n",
    "start_time = time.time()\n",
    "pca_en = PCA(n_components=200)\n",
    "x_en_pca = pca_en.fit_transform(encoded_en)\n",
    "train_features_en_pca, test_features_en_pca, train_labels_en_pca, test_labels_en_pca = train_test_split(x_en_pca, labels_en, random_state=42)\n",
    "svc_en_clf_pca = make_pipeline(StandardScaler(), SVC(gamma='auto'))\n",
    "svc_en_clf_pca.fit(train_features_en_pca,train_labels_en_pca)\n",
    "svc_en_pca_score=svc_en_clf_pca.score(test_features_en_pca, test_labels_en_pca)\n",
    "print(svc_en_pca_score)\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's train our Malay classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5685\n",
      "--- 20.03687310218811 seconds ---\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "import time\n",
    "start_time = time.time()\n",
    "svc_malay_clf = make_pipeline(StandardScaler(), SVC(gamma='auto'))\n",
    "svc_malay_clf.fit(train_features_new_malay,train_labels_new_malay)\n",
    "svc_malay_score=svc_malay_clf.score(test_features_new_malay, test_labels_new_malay)\n",
    "print(svc_malay_score)\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5675\n",
      "--- 4.90656042098999 seconds ---\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.svm import SVC\n",
    "import time\n",
    "start_time = time.time()\n",
    "pca_malay = PCA(n_components=200)\n",
    "x_malay_pca = pca_malay.fit_transform(encoded_malay)\n",
    "train_features_malay_pca, test_features_malay_pca, train_labels_malay_pca, test_labels_malay_pca = train_test_split(x_malay_pca, labels_malay, random_state=42)\n",
    "svc_malay_clf_pca = make_pipeline(StandardScaler(), SVC(gamma='auto'))\n",
    "svc_malay_clf_pca.fit(train_features_malay_pca,train_labels_malay_pca)\n",
    "svc_malay_pca_score=svc_malay_clf_pca.score(test_features_malay_pca, test_labels_malay_pca)\n",
    "print(svc_malay_pca_score)\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>sentence</th>\n",
       "      <th>lan</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3995</th>\n",
       "      <td>0</td>\n",
       "      <td>Blue Crush begitu berpanjangan dan membosankan...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3996</th>\n",
       "      <td>2</td>\n",
       "      <td>Kelegaan dari filem besbol yang terlalu keras ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3997</th>\n",
       "      <td>1</td>\n",
       "      <td>Orang miskin Stuart memerlukan sejumlah sinis ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3998</th>\n",
       "      <td>0</td>\n",
       "      <td>Tidak ada jumlah pembakaran, peledakan, tusuka...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3999</th>\n",
       "      <td>2</td>\n",
       "      <td>Berjaya mencapai apa yang dapat dilakukan oleh...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      label                                           sentence  lan\n",
       "3995      0  Blue Crush begitu berpanjangan dan membosankan...    1\n",
       "3996      2  Kelegaan dari filem besbol yang terlalu keras ...    1\n",
       "3997      1  Orang miskin Stuart memerlukan sejumlah sinis ...    1\n",
       "3998      0  Tidak ada jumlah pembakaran, peledakan, tusuka...    1\n",
       "3999      2  Berjaya mencapai apa yang dapat dilakukan oleh...    1"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test_new.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "def langdetect(text):\n",
    "    src='auto'##english, can chg to \\\"auto\\\" for language detection\n",
    "    dest='ms' #malay\n",
    "    url = \"https://clients5.google.com/translate_a/t?client=dict-chrome-ex&sl=\"+src+\"&tl=\"+dest+\"&q=\" + text\n",
    "    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/88.0.4324.104 Safari/537.36'}\n",
    "    try:\n",
    "        request_result = requests.get(url, headers=headers).json()     \n",
    "        #print(request_result)\n",
    "        #print('-------')\n",
    "        #print('[In Malay]: ' + request_result['alternative_translations'][0]['alternative'][0]['word_postproc'])\n",
    "        #result=request_result['alternative_translations'][0]['alternative'][0]['word_postproc'] #get translated result\n",
    "        result=request_result['src'] ## get the detected language\n",
    "        #print('[Language Dectected]: ' + request_result['src'])\n",
    "    except:\n",
    "        pass\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using Google Translate API to detect language with SST Mix dataset with 768 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Sentiment: 58.75 %\n",
      "Accuracy of Language: 99.78 %\n",
      "--- 591.8641953468323 seconds ---\n"
     ]
    }
   ],
   "source": [
    "#extract CLS token for every sentence\n",
    "match=0\n",
    "false=0\n",
    "false_lan=0\n",
    "match_lan=0\n",
    "i=0\n",
    "start_time = time.time()\n",
    "for i in range (0,len(df_test_new['sentence'])):\n",
    "    encoded_review = tokenizer.encode_plus(\n",
    "      df_test_new['sentence'][i],\n",
    "      max_length=512,\n",
    "      truncation=True,\n",
    "      add_special_tokens=True,\n",
    "      return_token_type_ids=False,\n",
    "      pad_to_max_length=True,\n",
    "      return_attention_mask=True,\n",
    "      return_tensors='pt',\n",
    "    )\n",
    "    input_ids = encoded_review['input_ids'].to(device)\n",
    "    attention_mask = encoded_review['attention_mask'].to(device)\n",
    "    with torch.no_grad():\n",
    "        last_hidden_states = model(input_ids, attention_mask=attention_mask)\n",
    "    feature = last_hidden_states\n",
    "    features = (feature[0][:,0,:].cpu()).numpy() #extract the last CLS token from BERT-layer\n",
    "    #prediction_language=int(svc_lan_clf.predict(features))\n",
    "    #print('Label:',prediction)\n",
    "    #print(f'Sentiment  : {class_names[prediction]}')\n",
    "    lan=langdetect(df_test_new['sentence'][i])\n",
    "    #print(lan)\n",
    "    if(lan=='en'):\n",
    "        prediction_language=0\n",
    "    elif(lan=='ms'):\n",
    "        prediction_language=1\n",
    "    else:\n",
    "        prediction_language=1\n",
    "    if (df_test_new['lan'][i]==prediction_language):\n",
    "        match_lan+=1\n",
    "        if(prediction_language==0):#en\n",
    "            prediction=int(svc_clf.predict(features))\n",
    "            if (df_test_new['label'][i]==prediction): \n",
    "                match+=1\n",
    "            elif(df_test_new['label'][i]!=prediction):\n",
    "                false+=1\n",
    "        elif(prediction_language==1): #malay\n",
    "            prediction=int(svc_malay_clf.predict(features))\n",
    "            if (df_test_new['label'][i]==prediction): \n",
    "                match+=1\n",
    "            elif(df_test_new['label'][i]!=prediction):\n",
    "                false+=1\n",
    "    elif(df_test_new['lan'][i]!=prediction_language):\n",
    "        false_lan+=1\n",
    "            \n",
    "\n",
    "accuracy_sentiment=((match)/len(df_test_new['sentence']))*100\n",
    "print('Accuracy of Sentiment:',\"{:.2f}\".format(round(accuracy_sentiment, 2)),'%')\n",
    "\n",
    "\n",
    "accuracy_language=(match_lan/len(df_test_new['sentence']))*100\n",
    "print('Accuracy of Language:',\"{:.2f}\".format(round(accuracy_language, 2)),'%')\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using Google Translate API to detect language with SST Mix dataset with PCA 200D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English misclassification: 806\n",
      "Malay misclassification: 857\n",
      "Accuracy of Sentiment: 58.20 %\n",
      "Accuracy of Language: 99.78 %\n",
      "--- 402.8547532558441 seconds ---\n"
     ]
    }
   ],
   "source": [
    "#extract CLS token for every sentence\n",
    "match=0\n",
    "match_en=0\n",
    "match_malay=0\n",
    "false=0\n",
    "false_en=0\n",
    "false_malay=0\n",
    "false_lan=0\n",
    "match_lan=0\n",
    "\n",
    "i=0\n",
    "start_time = time.time()\n",
    "for i in range (0,len(df_test_new['sentence'])):\n",
    "    encoded_review = tokenizer.encode_plus(\n",
    "      df_test_new['sentence'][i],\n",
    "      max_length=512,\n",
    "      truncation=True,\n",
    "      add_special_tokens=True,\n",
    "      return_token_type_ids=False,\n",
    "      pad_to_max_length=True,\n",
    "      return_attention_mask=True,\n",
    "      return_tensors='pt',\n",
    "    )\n",
    "    input_ids = encoded_review['input_ids'].to(device)\n",
    "    attention_mask = encoded_review['attention_mask'].to(device)\n",
    "    with torch.no_grad():\n",
    "        last_hidden_states = model(input_ids, attention_mask=attention_mask)\n",
    "    feature = last_hidden_states\n",
    "    features = (feature[0][:,0,:].cpu()).numpy() #extract the last CLS token from BERT-layer\n",
    "    #prediction_language=int(svc_lan_clf.predict(features))\n",
    "    #print('Label:',prediction)\n",
    "    #print(f'Sentiment  : {class_names[prediction]}')\n",
    "    lan=langdetect(df_test_new['sentence'][i])\n",
    "    #print(lan)\n",
    "    if(lan=='en'):\n",
    "        prediction_language=0\n",
    "    elif(lan=='ms'):\n",
    "        prediction_language=1\n",
    "    else:\n",
    "        prediction_language=1\n",
    "    if (df_test_new['lan'][i]==prediction_language):\n",
    "        match_lan+=1\n",
    "        if(prediction_language==0):#en\n",
    "            prediction=int(svc_en_clf_pca.predict(pca_en.transform(features)))\n",
    "            if (df_test_new['label'][i]==prediction): \n",
    "                match+=1\n",
    "                match_en+=1\n",
    "            elif(df_test_new['label'][i]!=prediction):\n",
    "                false+=1\n",
    "                false_en+=1\n",
    "        elif(prediction_language==1): #malay\n",
    "            prediction=int(svc_malay_clf_pca.predict(pca_malay.transform(features)))\n",
    "            if (df_test_new['label'][i]==prediction): \n",
    "                match+=1\n",
    "                match_malay+=1\n",
    "            elif(df_test_new['label'][i]!=prediction):\n",
    "                false+=1\n",
    "                false_malay+=1\n",
    "    elif(df_test_new['lan'][i]!=prediction_language):\n",
    "        false_lan+=1\n",
    "            \n",
    "print('English misclassification:', false_en)\n",
    "print('Malay misclassification:', false_malay)\n",
    "accuracy_sentiment=((match)/len(df_test_new['sentence']))*100\n",
    "print('Accuracy of Sentiment:',\"{:.2f}\".format(round(accuracy_sentiment, 2)),'%')\n",
    "\n",
    "\n",
    "accuracy_language=(match_lan/len(df_test_new['sentence']))*100\n",
    "print('Accuracy of Language:',\"{:.2f}\".format(round(accuracy_language, 2)),'%')\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
